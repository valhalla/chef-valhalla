#!/usr/bin/env python
from __future__ import print_function
import sys
import os
import stat
import math
import time
import boto
from filechunkio import FileChunkIO

#do some maintainance on s3, including pushing the new data up there
def push_to_s3(file_name, bucket_name = '<%= node[:valhalla][:bucket] %>', bucket_dir = '<%= node[:valhalla][:bucket_dir] %>'):
  #check what we have in s3
  s3 = boto.connect_s3()
  bucket = s3.get_bucket(bucket_name)
  basename = os.path.basename(file_name)
  basename, extension = os.path.splitext(basename)
  if basename == '' or extension == '':
    raise Exception('Sending extension-less or extension-only files is not allowed')
  prefix = basename[:basename.find('_') + 1]
  if prefix == basename[:-1]:
    raise Exception('No prefix detected for freshness culling, when naming use: prefix_$(date +%Y_%m_%d-%H_%M_%S).ext')
  keys = sorted([ k.key for k in bucket.get_all_keys() if k.key.startswith(bucket_dir + '/' + prefix) if k.key.endswith(extension) ])
  keys.reverse()

  #lets keep it to a managable amount of previous ones
  while len(keys) > 50:
    key = keys.pop()
    bucket.delete_key(key)

  #get ready for multi part
  source_size = os.stat(file_name).st_size
  key_name = bucket_dir + '/' + os.path.basename(file_name)
  multipart = bucket.initiate_multipart_upload(key_name)
  chunk_size = 536870912
  chunk_count = int(math.ceil(float(source_size) / float(chunk_size)))

  #send them
  print('Pushing %s to s3' % file_name)
  for i in range(chunk_count):
    offset = chunk_size * i
    bytes = min(chunk_size, source_size - offset)
    with FileChunkIO(file_name, 'r', offset=offset, bytes=bytes) as fp:
      multipart.upload_part_from_file(fp, part_num=i + 1)
      print('Part pushed to s3 %%%f' % (100 * float(offset + bytes) / float(source_size)))
  multipart.complete_upload()

  #check its ok and make it public
  key = bucket.get_key(key_name)
  if key is None:
    raise Exception('Failed to push file to s3')
  key.make_public()

#get all the instances of a layer and run some recipes on each
def update_instances(elb_name = '<%= node[:valhalla][:service_elb] %>', stack = '<%= node[:valhalla][:service_stack] %>', layer = '<%= node[:valhalla][:service_layer] %>', recipes = list('<%= node[:valhalla][:service_recipes] %>'.split(','))):
  #connect to opsworks
  opsworks = boto.connect_opsworks()

  #grab all the instances for that layer
  instances = opsworks.describe_instances(layer_id=layer)

  #TODO: make this more robust of a heuristic
  #TODO: sort by instance name so that we always try the same one first
  #if we dont have enough machines to feel safe we give up
  instances = [ i for i in instances['Instances'] if i.get('Status') == 'online' ]
  if len(instances) < <%= node[:valhalla][:min_service_update_instances] %>:
    raise Exception('Not enough instances in layer to risk the update')

  #connect to the elb and check which are in it
  elb = boto.connect_elb(elb_name)
  instances = [ i for i in instances if elb.describe_instance_health(elb_name, [i['Ec2InstanceId']])[0].state == 'InService' ]
  if len(instances) < <%= node[:valhalla][:min_service_update_instances] %>:
    raise Exception('Not enough instances in elb to risk the update')

  #for each one
  for instance in instances:
    #take this instance out of the elb
    registered = [ i.id for i in elb.deregister_instances(elb_name, [instance['Ec2InstanceId']]) ]
    if instance['Ec2InstanceId'] in registered:
      raise Exception('Instance was still registered with the elb')

    #check its status
    wait = 60
    while wait > 0:
      if elb.describe_instance_health(elb_name, [instance['Ec2InstanceId']])[0].state == 'OutOfService':
        break
      wait -= 5
      time.sleep(5)
    if wait < 1:
      elb.register_instances(elb_name, [instance['Ec2InstanceId']])
      raise Exception('Deregistered instance was not OutOfService')

    #deployment task of running a recipe
    deployment_id = opsworks.create_deployment(stack, {'Name': 'execute_recipes', 'Args': {'recipes': recipes}}, instance_ids=[instance['InstanceId']])['DeploymentId']
    #wait until it ends
    while True:
      if opsworks.describe_commands(deployment_id=deployment_id)['Commands'][0]['Status'] != 'pending':
        break
      time.sleep(5)
    #check that its status is all good
    if opsworks.describe_commands(deployment_id=deployment_id)['Commands'][0]['Status'] != 'successful':
      print('Deployment %s to Instance %s failed' % (deployment_id, instance['InstanceId']), file=sys.stderr) 
      sys.exit(1)

    #put the instance back in the elb
    registered = [ i.id for i in elb.register_instances(elb_name, [instance['Ec2InstanceId']]) ]
    if instance['Ec2InstanceId'] not in registered:
       raise Exception('Instance was not registered with the elb')

    #check its status
    wait = 60
    while wait > 0:
      if elb.describe_instance_health(elb_name, [instance['Ec2InstanceId']])[0].state == 'InService':
        break
      wait -= 5
      time.sleep(5)
    if wait < 1:
      raise Exception('Re-registered instance was not InService')
    

#entry point for script
if __name__ == "__main__":
  if len(sys.argv) < 2:
    print('Wrong arguments', file=sys.stderr)
    sys.exit(1)

  #for each arg
  for f in sys.argv[1:]:
    #check all the files are there
    if stat.S_ISREG(os.stat(f).st_mode) == False:
      print('%s is not a regular file') % f
    #push them to s3
    else:
      push_to_s3(f, '<%= node[:valhalla][:bucket] %>', '<%= node[:valhalla][:bucket_dir] %>')

  #update the service instances
  update_instances('<%= node[:valhalla][:service_elb] %>', '<%= node[:valhalla][:service_stack] %>', '<%= node[:valhalla][:service_layer] %>', list('<%= node[:valhalla][:service_recipes] %>'.split(',')))
